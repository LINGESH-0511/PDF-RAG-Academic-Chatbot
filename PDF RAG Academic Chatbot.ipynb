{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM78OKG5mD3yFwhRuTTnQjS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LINGESH-0511/PDF-RAG-Academic-Chatbot/blob/main/PDF%20RAG%20Academic%20Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet gradio faiss-cpu sentence-transformers pypdf requests\n"
      ],
      "metadata": {
        "id": "yKLt_FU-INNJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import faiss\n",
        "import requests\n",
        "from pypdf import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "import re\n",
        "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n"
      ],
      "metadata": {
        "id": "-rOv6WYhIRhx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "260ea108-e22b-45ed-93eb-f664b722abff",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GROQ_API_KEY = \"use your api key\"\n",
        "GROQ_ENDPOINT = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "GROQ_MODEL = \"llama-3.1-8b-instant\"\n",
        "\n"
      ],
      "metadata": {
        "id": "3DNq_Wm9IXDB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_multiple_pdfs(pdf_files):\n",
        "    chunks = []\n",
        "    metadata = []\n",
        "\n",
        "    for pdf in pdf_files:\n",
        "        reader = PdfReader(pdf.name)\n",
        "        for page_no, page in enumerate(reader.pages, start=1):\n",
        "            text = page.extract_text()\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            for i in range(0, len(text), 800):\n",
        "                chunk = text[i:i+800]\n",
        "                chunks.append(chunk)\n",
        "                metadata.append({\n",
        "                    \"pdf\": os.path.basename(pdf.name),\n",
        "                    \"page\": page_no\n",
        "                })\n",
        "\n",
        "    embeddings = embedding_model.encode(chunks)\n",
        "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "    index.add(embeddings)\n",
        "\n",
        "    return chunks, metadata, index\n"
      ],
      "metadata": {
        "id": "ZwyWE7APeGtE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_marks_and_type(question):\n",
        "    marks = re.findall(r\"\\b(2|3|5|10|15|20)\\b\", question)\n",
        "    marks = marks[0] if marks else \"general\"\n",
        "\n",
        "    q = question.lower()\n",
        "    if \"define\" in q:\n",
        "        qtype = \"definition\"\n",
        "    elif \"advantages\" in q or \"disadvantages\" in q:\n",
        "        qtype = \"pros_cons\"\n",
        "    elif \"compare\" in q:\n",
        "        qtype = \"comparison\"\n",
        "    elif \"summary\" in q:\n",
        "        qtype = \"summary\"\n",
        "    else:\n",
        "        qtype = \"general\"\n",
        "\n",
        "    return marks, qtype\n"
      ],
      "metadata": {
        "id": "W9yuRAZYIbo5",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_groq(prompt):\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": GROQ_MODEL,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are an academic expert assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"temperature\": 0.3,\n",
        "        \"max_tokens\": 900\n",
        "    }\n",
        "\n",
        "    response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        raise RuntimeError(response.text)\n",
        "\n",
        "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n"
      ],
      "metadata": {
        "id": "II1d94hsIerz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_answer(question, chunks, metadata, index, history, k=4):\n",
        "    q_embedding = embedding_model.encode([question])\n",
        "    _, indices = index.search(q_embedding, k)\n",
        "\n",
        "    context = \"\"\n",
        "    citations = []\n",
        "\n",
        "    for i in indices[0]:\n",
        "        context += chunks[i] + \"\\n\\n\"\n",
        "        meta = metadata[i]\n",
        "        citations.append(f\"{meta['pdf']} (Page {meta['page']})\")\n",
        "\n",
        "    marks, qtype = detect_marks_and_type(question)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Answer STRICTLY using the context.\n",
        "\n",
        "Question Type: {qtype}\n",
        "Marks: {marks}\n",
        "\n",
        "Formatting Rules:\n",
        "- 2 marks: short bullets\n",
        "- 10+ marks: headings + explanation\n",
        "- Summary: structured summary\n",
        "\n",
        "Previous Conversation:\n",
        "{history}\n",
        "\n",
        "Context:\n",
        "{context[:3000]}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "    answer = call_groq(prompt)\n",
        "    return answer, list(set(citations))\n"
      ],
      "metadata": {
        "id": "4x5ey2ozIgrg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_store = None\n",
        "metadata_store = None\n",
        "index_store = None\n",
        "chat_history_text = []\n",
        "\n",
        "def upload_pdfs(pdfs):\n",
        "    global chunks_store, metadata_store, index_store, chat_history_text\n",
        "    chat_history_text = []\n",
        "    chunks_store, metadata_store, index_store = process_multiple_pdfs(pdfs)\n",
        "    return \"‚úÖ PDFs indexed with source tracking\"\n",
        "def chat(question, history):\n",
        "    global chat_history_text\n",
        "\n",
        "    if not question.strip():\n",
        "        history.append((question, \"Please ask a valid question.\"))\n",
        "        return history, \"\"\n",
        "\n",
        "    try:\n",
        "        answer, citations = rag_answer(\n",
        "            question,\n",
        "            chunks_store,\n",
        "            metadata_store,\n",
        "            index_store,\n",
        "            \"\\n\".join(chat_history_text)\n",
        "        )\n",
        "\n",
        "        formatted = answer + \"\\n\\nüìö Sources:\\n\"\n",
        "        for c in citations:\n",
        "            formatted += f\"- {c}\\n\"\n",
        "\n",
        "        history.append((question, formatted))\n",
        "        chat_history_text.append(f\"Q: {question}\\nA: {answer}\")\n",
        "\n",
        "        return history, \"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        history.append((question, f\"‚ö†Ô∏è Error: {str(e)}\"))\n",
        "        return history, \"\"\n"
      ],
      "metadata": {
        "id": "Og4PgXL9IjEQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"## : üìÑ PDF RAG Academic Chatbott\")\n",
        "    gr.Markdown(\"Academic-grade PDF Question Answering with citations and adaptive answers.\")\n",
        "\n",
        "    pdf_input = gr.File(file_types=[\".pdf\"], file_count=\"multiple\", label=\"Upload PDFs\")\n",
        "    status = gr.Textbox(interactive=False)\n",
        "\n",
        "    pdf_input.change(upload_pdfs, inputs=pdf_input, outputs=status)\n",
        "\n",
        "    chatbot = gr.Chatbot(height=450, bubble_full_width=False)\n",
        "\n",
        "    msg = gr.Textbox(placeholder=\"Ask your academic question‚Ä¶\")\n",
        "\n",
        "    msg.submit(chat, inputs=[msg, chatbot], outputs=[chatbot, msg])\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "id": "LAsExlQVIlVZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "outputId": "57d0cbc7-7590-4ed0-b162-e1cff654c7d3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2444342488.py:1: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
            "/tmp/ipython-input-2444342488.py:10: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(height=450, bubble_full_width=False)\n",
            "/tmp/ipython-input-2444342488.py:10: DeprecationWarning: The 'bubble_full_width' parameter will be removed in Gradio 6.0. This parameter no longer has any effect.\n",
            "  chatbot = gr.Chatbot(height=450, bubble_full_width=False)\n",
            "/tmp/ipython-input-2444342488.py:10: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chatbot = gr.Chatbot(height=450, bubble_full_width=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8c352d52b565fc5f60.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8c352d52b565fc5f60.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}